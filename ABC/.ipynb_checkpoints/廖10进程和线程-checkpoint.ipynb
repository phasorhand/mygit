{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 进程和线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多任务的实现有3种方式：\n",
    "\n",
    "    多进程模式；\n",
    "    多线程模式；\n",
    "    多进程+多线程模式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线程是最小的执行单元，而进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。\n",
    "\n",
    "多进程和多线程的程序涉及到同步、数据共享的问题，编写起来更复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。\n",
    "\n",
    "子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\nprint('Process (%s) start...' % os.getpid())\\n# Only works on Unix/Linux/Mac:\\npid = os.fork()\\nif pid == 0:\\n    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))\\nelse:\\n    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以在Python程序中轻松创建爱你子进程：\n",
    "# Python os模块封装了常见的系统调用\n",
    "'''\n",
    "import os\n",
    "\n",
    "print('Process (%s) start...' % os.getpid())\n",
    "# Only works on Unix/Linux/Mac: windows无fork调用\n",
    "pid = os.fork()\n",
    "if pid == 0:\n",
    "    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))\n",
    "else:\n",
    "    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiprocessing模块》Process类代表一个进程对象，启动一个子进程并等待其结束："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent process 4296\n",
      "child process will start\n",
      "child process end.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "# 子进程需要执行的代码\n",
    "def run_proc(name):\n",
    "    print 'run child process %s %s...' % (name, os.getpid())\n",
    "    \n",
    "print 'parent process %s' % os.getpid()\n",
    "p = Process(target=run_proc, args=('test',))\n",
    "print 'child process will start'\n",
    "p.start()\n",
    "p.join()\n",
    "print 'child process end.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。\n",
    "\n",
    "join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要启动大量的子进程，可以用进程池的方式批量创建子进程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent process 4296\n",
      "waiting for all subprocesses done\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import os, time, random\n",
    "\n",
    "\n",
    "def long_time_task(name):\n",
    "    print 'run task %s %s ...' % (name, os.getpid())\n",
    "    start = time.time()\n",
    "    time.sleep(random.random()*3)\n",
    "    end = time.time()\n",
    "    print 'task %s runs %0.2f seconds.' % (name, (end - start))\n",
    "    \n",
    "print 'parent process %s' % os.getpid()\n",
    "p = Pool(4)\n",
    "for i in range(5):\n",
    "    p.apply_async(long_time_task, args=(i,))\n",
    "print 'waiting for all subprocesses done'\n",
    "p.close()\n",
    "p.join()\n",
    "print 'all done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：\n",
    "\n",
    "p = Pool(5)\n",
    "\n",
    "就可以同时跑5个进程。\n",
    "\n",
    "由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 子进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。\n",
    "\n",
    "subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。\n",
    "\n",
    "下面的例子演示了如何在Python代码中运行命令nslookup www.python.org，这和命令行直接运行的效果是一样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print 'nslookup www.python.org'\n",
    "r = subprocess.call(['nslookup', 'www.python.org'])\n",
    "print 'Exit code:', r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果子进程还需要输入，则可以通过communicate()方法输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print('$ nslookup')\n",
    "p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n')\n",
    "print(output.decode('utf-8'))\n",
    "print('Exit code:', p.returncode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码相当于在命令行执行命令nslookup，然后手动输入：\n",
    "\n",
    "set q=mx\n",
    "\n",
    "python.org\n",
    "\n",
    "exit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进程间通信"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Queue\n",
    "http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431927781401bb47ccf187b24c3b955157bb12c5882d000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在Unix/Linux下，可以使用fork()调用实现多进程。\n",
    "\n",
    "要实现跨平台的多进程，可以使用multiprocessing模块。\n",
    "\n",
    "进程间通信是通过Queue、Pipes等实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance = balance + n\n",
    "\n",
    "也分两步：\n",
    "\n",
    "    计算balance + n，存入临时变量中；\n",
    "    将临时变量的值赋给balance。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try...finally来确保锁一定会被释放。\n",
    "\n",
    "锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "balance = 0\n",
    "lock = threading.Lock()\n",
    "\n",
    "def run_thread(n):\n",
    "    for i in range(1000):\n",
    "        # 先获取锁\n",
    "        look.acquire()\n",
    "        try:\n",
    "            # 放心地修改\n",
    "            chagge_it(n)\n",
    "        finally:\n",
    "            # 一定要释放锁\n",
    "            lock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多核cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import threading, multiprocessing\n",
    "\n",
    "def loop():\n",
    "    x = 0\n",
    "    while True:\n",
    "        x = x ^ 1\n",
    "\n",
    "for i in range(multiprocessing.cpu_count()):\n",
    "    t = threading.Thread(target=loop)\n",
    "    t.start()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。\n",
    "\n",
    "但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？\n",
    "\n",
    "因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。\n",
    "\n",
    "不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。\n",
    "\n",
    "Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreadLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。\n",
    "\n",
    "可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。\n",
    "\n",
    "ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import threading\n",
    "\n",
    "# 创建全局ThreadLocal对象:\n",
    "local_school = threading.local()\n",
    "\n",
    "def process_student():\n",
    "    # 获取当前线程关联的student:\n",
    "    std = local_school.student\n",
    "    print('Hello, %s (in %s)' % (std, threading.current_thread().name))\n",
    "\n",
    "def process_thread(name):\n",
    "    # 绑定ThreadLocal的student:\n",
    "    local_school.student = name\n",
    "    process_student()\n",
    "\n",
    "t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')\n",
    "t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 进程vs线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。\n",
    "\n",
    "如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。\n",
    "\n",
    "如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。\n",
    "\n",
    "多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。\n",
    "\n",
    "所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算密集型 vs. IO密集型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。\n",
    "\n",
    "计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。\n",
    "\n",
    "第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。\n",
    "\n",
    "IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 异步IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。\n",
    "\n",
    "对应到Python语言，单进程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。\n",
    "\n",
    "举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？\n",
    "\n",
    "原有的Queue可以继续使用，但是，通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。\n",
    "\n",
    "我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# task_master.py\n",
    "\n",
    "import random, time, queue\n",
    "from mutiprocessing.managers import BaseManager\n",
    "\n",
    "# 发送任务的队列：\n",
    "task_queue = queue.Queue()\n",
    "# 接收结果的队列：\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# 从BaseManager继承的QueueManager\n",
    "class QueueManager(BaseManager):\n",
    "    pass\n",
    "\n",
    "# 把两个Queue都注册到网络上， callable参数关联了Queue对象：\n",
    "QueueManager.register('get_task_queue', callable=lambda: task_queue)\n",
    "QueueManager.register('get_result_queue', callable=lambda: result_queue)\n",
    "# 绑定端口5000， 设置验证码’abc'\n",
    "manager = QueueManager(address=('', 5000, authkey=b'abc'))\n",
    "# 启动Queue\n",
    "manager.start()\n",
    "# 获得通过网络访问的Queue对象\n",
    "task = manager.get_task_queue()\n",
    "result = manager.get_result_queue()\n",
    "# 放几个任务进去\n",
    "for i in range(10):\n",
    "    n = random.randint(0, 10000)\n",
    "    print 'Put task %d' % n\n",
    "    task.put(n)\n",
    "# 从result队列读取结果：\n",
    "print 'Try get result...'\n",
    "for i in range(10):\n",
    "    r = result.get(timeout=10)\n",
    "    print 'Result: %s' % r\n",
    "    \n",
    "# 关闭：\n",
    "manager.shutdown()\n",
    "print 'master exit.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得的Queue接口添加。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，在另一台机器上启动任务进程（本机上启动也可以）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# task_worker.py\n",
    "\n",
    "import time, sys, queue\n",
    "from multiprocessing.managers import BaseManager\n",
    "\n",
    "# 创建类似的QueueManager：\n",
    "class QueueManager(BaseManager):\n",
    "    pass\n",
    "\n",
    "# 由于这个QueueManager只能从网络上获取Queue，所以注册时值提供名字：\n",
    "QueueManager.register('get_task_queue')\n",
    "QueueManager.register('get_result_queue')\n",
    "\n",
    "# 连接到服务器，也就是运行task_master.py的机器：\n",
    "server_addr = '127.0.0.1'\n",
    "pritn 'Connect to server %s' server_addr\n",
    "# 端口和验证码注意和task_master.py设置的完全一样：\n",
    "m = QueueManager(address=(server_addr, 5000), authkey=b'abc')\n",
    "# 从网络连接：\n",
    "m.connect()\n",
    "# 获取Queue的对象：\n",
    "task = m.get_task_queue()\n",
    "result = get_result_queue()\n",
    "# 从task队列取任务，并把结果写入result队列\n",
    "for i in range(10):\n",
    "    try:\n",
    "        n = task.get(timeout=1)\n",
    "        print 'run task %d * %d' % (n, n)\n",
    "        r = '%d * %d' % (n, n, n*n)\n",
    "        time.sleep(1)\n",
    "        result.put(r)\n",
    "    except Queue.Empty:\n",
    "        print 'task queue is empty.'\n",
    "        \n",
    "# 处理结束\n",
    "print 'worker exit.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。\n",
    "\n",
    "现在，可以试试分布式进程的工作效果了。先启动task_master.py服务进程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
